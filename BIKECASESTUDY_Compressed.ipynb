{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color= \"green\"> This is Compressed Folder please execute all the cells below to check the results of EDA and model Building results in your local machine </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "  - A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n",
    "\n",
    "\n",
    "- A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n",
    "\n",
    "\n",
    "- In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n",
    "\n",
    "\n",
    "- They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. \n",
    "\n",
    "\n",
    "- The company wants to know: Which variables are significant in predicting the demand for shared bikes.How well those variables describe the bike demands. Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image1.jfif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading the day.csv data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from day.csv \n",
    "data=pd.read_csv('day.csv')\n",
    "\n",
    "# Displaying the first 5 rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the number of rows and columns in the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying data typesa ling with non-null count of all columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences: \n",
    "    - Null values are not present in the data\n",
    "    - Some of the categorical columns like [season,yr,mnth,holiday,weekday,workingday,weathersit] are treated as dataype int which we need to convert to object in further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences:\n",
    "\n",
    "    - To be confident we executed this command which says, We dont have any null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================\n",
    "Dataset characteristics\n",
    "=========================================\t\n",
    "day.csv have the following fields:\n",
    "\t\n",
    "\t- instant: record index\n",
    "\t- dteday : date\n",
    "\t- season : season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "\t- yr : year (0: 2018, 1:2019)\n",
    "\t- mnth : month ( 1 to 12)\n",
    "\t- holiday : weather day is a holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "\t- weekday : day of the week\n",
    "\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\t+ weathersit : \n",
    "\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\t- temp : temperature in Celsius\n",
    "\t- atemp: feeling temperature in Celsius\n",
    "\t- hum: humidity\n",
    "\t- windspeed: wind speed\n",
    "\t- casual: count of casual users\n",
    "\t- registered: count of registered users\n",
    "\t- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Treating unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing instant column from the dataset since its a record index and doesn't help in our analysis\n",
    "data.drop('instant',axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the columns after removal of 'instant' column\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Not consider dta(date) in this analysyis. Since the day and month has been already explained by mnth and yr columns. Considering this may be lead to multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the dteday column using index iloc\n",
    "data=data.iloc[:,1:]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 1: Verify that Data has linear Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to understand the relationship between varibales in the data \n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - We observed that there is a linear relationship between temp and cnt columns,which is good sign for our prediction\n",
    "    - we oberved that temp and atemp has high degree of positive correltation betwen them almost a straight line which indiactes high linear relationship, Dropping one of the column is a good practice to avoid multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise the categorical columns\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(3,3,1)\n",
    "plt.title('Distribution of Season vs cnt')\n",
    "sns.boxplot('season','cnt',data=data)\n",
    "\n",
    "plt.subplot(3,3,2)\n",
    "plt.title('Distribution of year vs cnt')\n",
    "sns.boxplot('yr','cnt',data=data)\n",
    "\n",
    "plt.subplot(3,3,3)\n",
    "plt.title('Distribution of month vs cnt')\n",
    "sns.boxplot('mnth','cnt',data=data)\n",
    "\n",
    "plt.subplot(3,3,4)\n",
    "plt.title('Distribution of holiday vs cnt')\n",
    "sns.boxplot('holiday','cnt',data=data)\n",
    "\n",
    "plt.subplot(3,3,5)\n",
    "plt.title('Distribution of weekday vs cnt')\n",
    "sns.boxplot('weekday','cnt',data=data)\n",
    "\n",
    "plt.subplot(3,3,6)\n",
    "plt.title('Distribution of workingday vs cnt')\n",
    "sns.boxplot('workingday','cnt',data=data)\n",
    "\n",
    "plt.subplot(3,3,7)\n",
    "plt.title('Distribution of weathersit vs cnt')\n",
    "sns.boxplot('weathersit','cnt',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MultiCollinearity Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating temp and atemp Features\n",
    "    - temp : temperature in Celsius\n",
    "    - atemp: feeling temperature in Celsius\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets understand correlations of numerical columns\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(data.corr(),cmap='gray',annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights\n",
    "\n",
    "    - We comuld observe that temp and atemp are correlated so we can drop one of the columns to avoid multicollinearity\n",
    "    - We observed similar relationship when we did visulisation using pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop atemp column from the dataset\n",
    "data.drop(['atemp'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming that atemp column has been dropped by using dataframe.head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating where datatype is integer\n",
    "\n",
    "    - casual: count of casual users\n",
    "    - registered: count of registered users\n",
    "    - cnt: count of total rental bikes including both casual and registered\n",
    "    \n",
    "### Since cnt(count of total rental bikes) is equal to sum of casual and registered column,we can consider cntcolumn and drop casual and registered columns so that we can avoid multi collinearity in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since casual and registered sum is equal to cnt we can drop them to avoid multicollinearity\n",
    "\n",
    "data.drop(['casual','registered'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming that the selected  columns are dropped\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption: The independent variables should not be correlated absence of this phenomenon may leads to Multicollinearity. \n",
    "\n",
    "    - we have removed atemp since this column(independent variable) is correlated with temp variable    \n",
    "    - We have removed casual and registered column since sum of these two independent features results in cnt column which is our target column and may mislead our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between numerical columns\n",
    "\n",
    "numerical_columnscorr=data[['temp','hum','windspeed','cnt']]\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(numerical_columnscorr.corr(),cmap='gray',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences:\n",
    "\n",
    "    - cnt(Target) column has highest correlation with temp column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating season column which was encoded as 1,2,34 in the given dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert season column to their season names\n",
    "def func_season(x):\n",
    "    if x==1:\n",
    "        return 'spring'\n",
    "    elif x==2:\n",
    "        return 'summer'\n",
    "    elif x==3:\n",
    "        return 'fall'\n",
    "    elif x==4:\n",
    "        return 'winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply above function on season column\n",
    "data['season']=data['season'].apply(func_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the change mad in the previous step\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Month column which was encoded from 1-12 in the serial order of monthwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat mnth(month) column which was given as encoded format in dataset as 1 to 12\n",
    "\n",
    "def func_mnth(x):\n",
    "    return x.map({1:'jan',2:'feb',3:'mar',4:'apr',5:'may',6:'june',7:'july',8:'aug',9:'sep',10:'oct',11:'nov',12:'dec'})\n",
    "\n",
    "# convert month column with the name of months which was given as 1-12 in our dataset\n",
    "data[['mnth']]=data[['mnth']].apply(func_mnth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets treat categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a univariate_analysis function which helps in analysing uivariate analysis\n",
    "\n",
    "def univariate_analysis(feature):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Count of different categories in '+feature)\n",
    "    sns.countplot(feature,data=data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on Season column\n",
    "\n",
    "univariate_analysis('season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on mnth(month) column\n",
    "univariate_analysis('mnth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on holiday column\n",
    "univariate_analysis('holiday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on weekday column\n",
    "univariate_analysis('weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on workingday column\n",
    "univariate_analysis('workingday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on weatersit column\n",
    "univariate_analysis('weathersit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets understand correlations of numerical columns\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(data.corr(),cmap='gray',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights from heatmap\n",
    "\n",
    "- cnt(Target variable) is explained by temp and atemp variables with same coorelation so we can drop one of the columns \n",
    "- windspeed is negatively correlated with cnt, which is quite obvious when there is a high windspeed bikerides re not preferrable by many people, Further analysis is required in the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatype Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the datatype of a column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Noticed from dataDictionary that the following columns are categorical but the datatype of this colmns are int in the dataset and we need to treat them:\n",
    " \n",
    " - yr\n",
    " - holiday\n",
    " - weekday\n",
    " - workingday\n",
    " - weathersit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets convert datatype into datatype:object where column is mentioned as int but actually they are categorical in nature\n",
    "\n",
    "data['yr']=data['yr'].astype('object')\n",
    "data['holiday']=data['holiday'].astype('object')\n",
    "data['weekday']=data['weekday'].astype('object')\n",
    "data['workingday']=data['workingday'].astype('object')\n",
    "data['weathersit']=data['weathersit'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the datatype of a column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have converted the categorical datatypes to the required datatype,Lets roceed with further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consoder variable y as Target column which is cnt(count) of bikes\n",
    "\n",
    "y=data.iloc[:,-1:]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping cnt column from the dataset since it has been taken as Target column\n",
    "data.drop('cnt',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical column Treatment in Feature/input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical columns are formed in the new dataframe which is data_categorical\n",
    "data_categorical=data.iloc[:,0:7]\n",
    "data_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Categorical columns are created in a new dataframe. i.e..,data_noncategorical\n",
    "\n",
    "data_noncategorical=data.iloc[:,7:]\n",
    "data_noncategorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns are treated using dummy varibale creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using pandas inbuilt method which is get_dummies() method to treat the categorical column.\n",
    "\n",
    "data_categorical=pd.get_dummies(data_categorical,drop_first=True)\n",
    "data_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences:\n",
    "\n",
    "    - We could observe that get_dummies() has been applied on categorical columns\n",
    "    - drop_first=True has dropped one of the category in eacch feature to avoid dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the shape of categorical and non-categoricla columns in our Input/feature/x\n",
    "print(data_categorical.shape)\n",
    "print(data_noncategorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets combine two dataframes data_1 and data_2 since caetgorical columns tretament has been performed\n",
    "\n",
    "final_data=pd.concat([data_categorical,data_noncategorical],axis=1)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(final_data.corr(),annot=True,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Testing Sets for model building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split from sklearn.model_selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets consider 80 percent of our data into trainning data and 20 percent to test data\n",
    "x_train,x_test,y_train,y_test=train_test_split(final_data,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the shape after train and test split\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying min max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to apply scaling on three independent variables where the column datatype is int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appply min max scaling which scales the value between 0 and 1\n",
    "# Final_data is the name given to dataframe and this will be passed to model after scaling is applied\n",
    "\n",
    "final_data[['temp','hum','windspeed']]=mm.fit_transform(final_data[['temp','hum','windspeed']])\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at description to see that min and max has been applied on the numerical columns which we applied using min max scaler\n",
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling on target column. i.e..,y/Dependent variable\n",
    "y=mm.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Linearregression model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the object\n",
    "lr=LinearRegression()\n",
    "\n",
    "# Fitting the model\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying prediction\n",
    "y_pred=lr.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intercept and coefficient of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept and coefficient of our model\n",
    "# In multi linear regression line we had multiple coefficient as shown in the output below and one Intercept .i.e.., The point at y=0 line cuts the y-axis\n",
    "\n",
    "print(\"Intercept of ou model is: \",lr.intercept_)\n",
    "print()\n",
    "print(\"Coefficient of our model is: \",lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see another analysis where error terms should be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred is used to predict output on the training data\n",
    "y_train_pred=lr.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding residual, to understand difference between fitted value and predicted value\n",
    "residual=y_train-y_train_pred\n",
    "residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption: Error Terms should be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ditplot is used to analyse the distribution of Residual terms\n",
    "\n",
    "plt.title('Distribution of Error terms')\n",
    "sns.distplot(residual)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences:\n",
    "\n",
    "    - We could see that Error terms are normally distributed with almost/closely mean equal to 0.Now lets Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import r2_score from sklearn.metrics\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Squared score \n",
    "\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrcis Evaluation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "print(\"Mean squared Error: \",mean_squared_error(y_test,y_pred))\n",
    "print(\"Mean Absolute Error: \",mean_absolute_error(y_test,y_pred))\n",
    "print(\"Roor mean squared Error: \",(np.sqrt(mean_squared_error(y_test,y_pred))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences:\n",
    "\n",
    "    - Its a good sign to look at MeanSquaredError and MeanAbsoluteError as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred using sactter plot to understand the spread of the Target variables\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('y_test vs y_pred')\n",
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel('y_test', fontsize=15)                          \n",
    "plt.ylabel('y_pred', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "\n",
    "        - y_test and y_pred are linearly related as shown above which indicates model has performed well in predicting        on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see Adjusted R2 value which is a good metric for multiple linear regression analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r2 score on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2 score on train data\n",
    "yhat=lr.predict(x_train)                                # yhat is predicted value\n",
    "RSS=np.sum((yhat-y_train)**2)                           # rss=(yhat-y)**2\n",
    "TSS=np.sum((np.mean(yhat)-y_train)**2)\n",
    "r2=1-(RSS/TSS)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r2 score on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2 score on test data\n",
    "\n",
    "yhat=lr.predict(x_test)                    # y-hat here is predicted value on test data\n",
    "RSS=np.sum((yhat-y_test)**2)\n",
    "TSS=np.sum((np.mean(yhat)-y_test)**2)\n",
    "r2=1-(RSS/TSS)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted r2 on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted r2 on training data\n",
    "\n",
    "yhat = lr.predict(x_train)                             # y-hat here is predicted value on train data\n",
    "SumSquaresResidual = np.sum((y_train-yhat)**2)\n",
    "SumSquaresTotal = np.sum((y_train-np.mean(y_train))**2)\n",
    "r_squared = 1 - (float(SumSquaresResidual))/SumSquaresTotal\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y_train)-1)/(len(y_train)-x_train.shape[1]-1)\n",
    "print(r_squared, adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted r2 on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted r2 on test data\n",
    "\n",
    "yhat = lr.predict(x_test)                              # y-hat here is predicted value on test data\n",
    "SumSquaresResidual = np.sum((y_test-yhat)**2)\n",
    "SumSquaresTotal = np.sum((y_test-np.mean(y_test))**2)\n",
    "r_squared = 1 - (float(SumSquaresResidual))/SumSquaresTotal\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)\n",
    "print(r_squared, adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "\n",
    "    - Adjusted r2 is considered as better metrics in Multiple Linear regression considering that we have obtained 0.834375 in train data\n",
    "    - Adjusted r2 for test datais 0.825761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences on Overall model Building,evaluation and Prediction:\n",
    "\n",
    "    - Adjusted r2 is good measure for Multiple linear regression,since we have multiple independent variables\n",
    "    - Adjusted r2 on train data is: 0.834375\n",
    "    - Adjusted r2 on test data is 0.825761\n",
    "    - We could see that Error terms are normally distributed with almost/closely mean equal to 0.Now lets Evaluate our model\n",
    "    - the plot we have drawn on y_test and y_pred shows a good linear relation,which indicates good model perfomance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Stats model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stats models library for statistical analysis\n",
    "\n",
    "import statsmodels.api as sm   \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a user defined functin called statsmodel_analysis for furter stats analysis,which will reduce the lines of code\n",
    "\n",
    "def statsmodel_analysis(x,y):\n",
    "    x=sm.add_constant(x)                 # step 1: Add cnstant\n",
    "    result=sm.OLS(y,x).fit()             # step 2:fit the model\n",
    "    print(result.summary())              # step 3: return the summary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a user defined functin called checkVIF for furter stats analysis,which will reduce the lines of code\n",
    "\n",
    "def MeasureVIF(x):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = x.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering all the columns where adjusted r2 is 0.834 which has been noticed by performing model evaluatin using sklearn\n",
    "\n",
    "x_train_stats1=x_train\n",
    "statsmodel_analysis(x_train_stats1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "\n",
    "    - Pvalue for weekday_2 is 0.987 lets remove this since its statistically insignificant in our model\n",
    "    - Highest VIF:172.63 has been noticed for workingday_1\n",
    "    - Adjusted r2 : 0.834"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday_2 has high p-value greater than 0.05 lets drop this column from our analysis\n",
    "\n",
    "# x_train_stats2 dataframe doesn't hold weekday_2 column/feature\n",
    "\n",
    "x_train_stats2=x_train_stats1.drop(['weekday_2'],axis=1)\n",
    "statsmodel_analysis(x_train_stats2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights;\n",
    "\n",
    "    - PValue for mnth_june is 0.966 lets remove this since its statistically insignificant in our model\n",
    "    - VIF for temp is 44.75\n",
    "    - Adjusted r2 :0.835 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mnth_june column where p-value is 0.966 which is statistically not significant\n",
    "\n",
    "# x_train_stats3 dataframe doens't hold weekday_2 and weekday_1 column/feature\n",
    "\n",
    "x_train_stats3=x_train_stats1.drop(['weekday_2','mnth_june'],axis=1)\n",
    "statsmodel_analysis(x_train_stats3,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - pvalue of weekday_1 is 0.796 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is highest at 37.77\n",
    "    - Adjusted r2 : 0.835\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weekday_1 column where p-value is 0.796 which is statistically not significant\n",
    "\n",
    "# x_train_stats4 dataframe doens't hold weekday_2, weekday_1 and mnth_june\n",
    "\n",
    "x_train_stats4=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1'],axis=1)\n",
    "statsmodel_analysis(x_train_stats4,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "    - PValue of mnth_feb is 0.770 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is highest at 37.76\n",
    "    - Adjusted r2 : 0.835"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mnth_feb column where p-value is 0.770 which is statistically not significant\n",
    "\n",
    "# x_train_stats5 dataframe doens't hold weekday_2, mnth_june,weekday_1 and mnth_feb columns/feature\n",
    "\n",
    "x_train_stats5=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb'],axis=1)\n",
    "statsmodel_analysis(x_train_stats5,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.836\n",
    "    - pvalue of mnth_aug is 0.694 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is high at 34.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mnth_aug column where p-value is 0.694 which is statistically not significant\n",
    "\n",
    "# x_train_stats6 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb and mnth_aug columns/feature\n",
    "\n",
    "x_train_stats6=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug'],axis=1)\n",
    "statsmodel_analysis(x_train_stats6,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.836\n",
    "    - pvalue of mnth_jan is 0.415 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is high at 33.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mnth_jan column where p-value is 0.415 which is statistically not significant\n",
    "\n",
    "# x_train_stats7 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb, mnth_aug and mnth_jan columns/feature\n",
    "\n",
    "x_train_stats7=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan'],axis=1)\n",
    "statsmodel_analysis(x_train_stats7,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.836\n",
    "    - pvalue of weekday_3 is 0.288 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is high at 32.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats modle 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weekday_3 column where p-value is 0.288 which is statistically not significant\n",
    "\n",
    "# x_train_stats8 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb, mnth_aug,mnth_jan and weekday_3 columns/feature\n",
    "\n",
    "x_train_stats8=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3'],axis=1)\n",
    "statsmodel_analysis(x_train_stats8,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.836\n",
    "    - pvalue of weekday_6 is 0.248 we can remove this feature,Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is high at 32.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weekday_6 column where p-value is 0.248 which is statistically not significant\n",
    "\n",
    "# x_train_stats9 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb, mnth_aug,mnth_jan,weekday_3 and weekday_6 columns/feature\n",
    "\n",
    "x_train_stats9=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3','weekday_6'],axis=1)\n",
    "statsmodel_analysis(x_train_stats9,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.836\n",
    "    - pvalue of mnth_oct is 0.209 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is high at 32.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodel 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mnth_oct column where p-value is 0.209 which is statistically not significant\n",
    "\n",
    "# x_train_stats10 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb, mnth_aug,mnth_jan,weekday_3,weekday_6 and mnth_oct columns/feature\n",
    "\n",
    "x_train_stats10=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3','weekday_6','mnth_oct'],axis=1)\n",
    "statsmodel_analysis(x_train_stats10,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.836\n",
    "    - pvalue of season_summer is 0.257 we can remove this feature, Since high p-value indicates feature is insignificant in model\n",
    "    - VIF for hum is at 31.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodel 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop season_summer  column where p-value is 0.257 which is statistically not significant\n",
    "\n",
    "# x_train_stats11 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb, mnth_aug,mnth_jan,weekday_3,weekday_6, mnth_oct and season_summer columns/feature\n",
    "\n",
    "x_train_stats11=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3','weekday_6','mnth_oct','season_summer'],axis=1)\n",
    "statsmodel_analysis(x_train_stats11,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.835\n",
    "    - PValue of all the features/columns is less than 0.05 which indiactes all the features are significant to the model\n",
    "    - VIF for hum(humidity) is 28.79 which is the highest VIF among all other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop humidity(hum)  column whereVIF is 28.79 which may lead to multicollinearity and statistically insignificant\n",
    "\n",
    "# x_train_stats12 dataframe doesn't hold weekday_2, mnth_june,weekday_1 ,mnth_feb, mnth_aug,mnth_jan,weekday_3,weekday_6, mnth_oct,season_summer and hum columns/feature\n",
    "\n",
    "x_train_stats12=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3','weekday_6','mnth_oct','season_summer','hum'],axis=1)\n",
    "statsmodel_analysis(x_train_stats12,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeasureVIF(x_train_stats12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    - Adjusted r-squared is 0.830\n",
    "    - PValue of all the features/columns is less than 0.05 which indiactes all the features are significant to the model\n",
    "    - VIF for temp is 7.93 which is the highest VIF among all other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## consider new Features which are statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We consider the features obtained in stats model 12 and proceed with model  building to check the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_selected features hold the features which are statistically significant \n",
    "\n",
    "x_train_selectedfeatures=x_train_stats1.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3','weekday_6','mnth_oct','season_summer','hum'],axis=1)\n",
    "x_train_selectedfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_selectedfeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are left with 17 columns lets consider the 17 clumns for test data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test selected features\n",
    "x_testselectedfeatures=x_test.drop(['weekday_2','mnth_june','weekday_1','mnth_feb','mnth_aug','mnth_jan','weekday_3','weekday_6','mnth_oct','season_summer','hum'],axis=1)\n",
    "x_testselectedfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train_selectedfeatures,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predselectedfeatures=lr.predict(x_testselectedfeatures)\n",
    "y_predselectedfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred using sactter plot to understand the spread of the Target variables\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test,y_predselectedfeatures)\n",
    "plt.xlabel('y_test', fontsize=15)                          \n",
    "plt.ylabel('y_predselectedfeatures', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted r2 on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted r2 on training data\n",
    "\n",
    "yhat = lr.predict(x_train_selectedfeatures)                             \n",
    "SumSquaresResidual = np.sum((y_train-yhat)**2)\n",
    "SumSquaresTotal = np.sum((y_train-np.mean(y_train))**2)\n",
    "r_squared = 1 - (float(SumSquaresResidual))/SumSquaresTotal\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y_train)-1)/(len(y_train)-x_train_selectedfeatures.shape[1]-1)\n",
    "print(r_squared, adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted r2 on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted r2 on test data\n",
    "\n",
    "yhat = lr.predict(x_testselectedfeatures)                              \n",
    "SumSquaresResidual = np.sum((y_test-yhat)**2)\n",
    "SumSquaresTotal = np.sum((y_test-np.mean(y_test))**2)\n",
    "r_squared = 1 - (float(SumSquaresResidual))/SumSquaresTotal\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-x_testselectedfeatures.shape[1]-1)\n",
    "print(r_squared, adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions performed above:\n",
    "\n",
    "    - Importing and reading data\n",
    "    - Finding Missing values    \n",
    "    - Univariate and Bivariate Analysis for better insights of data\n",
    "    - Feature selection\n",
    "    - Considered the Linear regression Assumptions\n",
    "    - Model Building\n",
    "    - Model evaluation\n",
    "    - Statistical Analysis to avoid multicollinearity and finding best features of the data\n",
    "    - Verifying the accuracy after training the model on the new features which was considered after perfomring statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result Comparison between Train model and Test:\n",
    "    \n",
    "- Train Adjusted R^2 : 0.835\n",
    "- Test Adjusted R^2  : 0.847\n",
    "- Difference in Adjusted R^2 between Train and Test data is : 1 % which is less than 5% indicates a **Good model**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences on Overall Stats model Building:\n",
    "   - As observed in the analysis, We can see more demand for bikes in 2019 than 2018, Which indicates the business is gradually increasing the revenue over the days. So business can focus more on other variabes and can plan accordingly afetr the pandemic.\n",
    "   - Business Can focus more on Summer & Winter season, March,May and  September month they have good influence on bike rentals.\n",
    "   - We can see spring season has negative coefficients and negatively correlated to bike rentals. So we can provide offers during this time to increase the revenue and attract customers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
